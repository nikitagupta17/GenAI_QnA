# Questions on Evaluation Methods

## 1. What are some common evaluation metrics used in NLP, and how do you decide which one to use?

**Answer:**
**Classification:** Accuracy, precision, recall, F1-score for balanced evaluation of classification tasks.

**Generation:** BLEU, ROUGE for reference-based evaluation, perplexity for language modeling quality.

**Semantic:** BERTScore, embedding similarity for meaning preservation.

**Selection Criteria:** Task type (classification vs generation), availability of references, interpretability needs, computational cost.

## 2. How do you approach model evaluation differently for generative AI tasks like text generation versus classification tasks?

**Answer:**
**Classification:** Use standard metrics (accuracy, F1), confusion matrices, ROC curves for binary classification.

**Generative Tasks:** Require reference-free metrics, human evaluation, diversity measures, factual accuracy assessment.

**Key Differences:** Generative tasks need creativity and coherence evaluation, multiple valid outputs exist, harder to automate evaluation.

**Approach:** Combine automated metrics with human evaluation for comprehensive generative model assessment.

## 3. What is the importance of human evaluation in NLP, especially for generative AI?

**Answer:**
**Necessity:** Automated metrics can't capture nuanced quality aspects like creativity, appropriateness, and contextual relevance.

**Evaluation Aspects:** Fluency, coherence, relevance, factual accuracy, style appropriateness, safety considerations.

**Implementation:** Expert evaluation panels, crowdsourced evaluation, user studies, A/B testing.

**Challenges:** Cost, scalability, inter-annotator agreement, bias in human judgment.

## 4. How do you evaluate models for bias and fairness, especially in NLP tasks?

**Answer:**
**Bias Detection:** Test performance across demographic groups, analyze output sentiment by group, check for stereotypical associations.

**Fairness Metrics:** Equalized odds, demographic parity, individual fairness measures.

**Evaluation Process:** Create diverse test sets, measure disparate impact, analyze attention patterns for bias.

**Mitigation:** Regular bias audits, diverse evaluation teams, counterfactual testing, fairness-aware training.

## 5. What is perplexity, and why is it used to evaluate language models?

**Answer:**
Perplexity measures how well a language model predicts a sequence of words, calculated as the exponential of average negative log-likelihood.

**Formula:** PP(W) = exp(-1/N × Σ log P(w_i|w_1...w_{i-1}))

**Interpretation:** Lower perplexity indicates better prediction ability and language understanding.

**Limitations:** Doesn't capture semantic understanding, factual accuracy, or task-specific performance quality.

## 6. How do you evaluate the coherence and relevance of text generated by an NLP model?

**Answer:**
**Coherence:** Measure logical flow, consistency, and readability using automated coherence models or human evaluation.

**Relevance:** Assess how well generated text addresses the input query or context using semantic similarity measures.

**Automated Methods:** Coherence scoring models, discourse relation analysis, topic consistency measures.

**Human Evaluation:** Likert scale ratings, comparative evaluation, expert assessment for domain-specific content.

## 7. Discuss metrics like BLEU, METEOR, and human evaluation for coherence and relevance, particularly in conversational AI or creative text generation.

**Answer:**
**BLEU:** Measures n-gram overlap with references, good for translation but limited for creative tasks.

**METEOR:** Considers synonyms and word order, better than BLEU for semantic similarity.

**Human Evaluation:** Essential for coherence, creativity, and appropriateness in conversational AI.

**Conversational AI:** Requires turn-level and dialogue-level evaluation, context appropriateness, engagement metrics.

## 8. What methods can be used to assess the diversity of generated text?

**Answer:**
**Lexical Diversity:** Unique n-gram ratios, vocabulary richness measures, repetition analysis.

**Semantic Diversity:** Embedding-based similarity analysis, topic distribution measures.

**Structural Diversity:** Sentence length variation, syntactic pattern diversity.

**Implementation:** Self-BLEU (lower is more diverse), distinct n-gram ratios, semantic clustering analysis.

## 9. What role does prompt engineering play in evaluation, especially for models like GPT?

**Answer:**
**Critical Role:** Prompt quality significantly affects model performance, evaluation results depend heavily on prompt design.

**Considerations:** Prompt consistency across evaluations, prompt bias effects, few-shot example selection.

**Best Practices:** Standardize prompts for fair comparison, test multiple prompt variations, consider prompt sensitivity.

**Evaluation:** Include prompt effectiveness as part of model evaluation, optimize prompts for specific tasks.

## 10. What are ROUGE scores, and why are they commonly used for summarization?

**Answer:**
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures overlap between generated and reference summaries.

**Variants:** ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence), ROUGE-W (weighted longest common subsequence).

**Why for Summarization:** Captures content preservation, measures information coverage, provides automatic evaluation.

**Limitations:** Doesn't capture semantic meaning, requires reference summaries, may penalize valid alternative phrasings.

## 11. Explain the ROUGE metric and its variants (ROUGE-N, ROUGE-L) as measures of overlap between model-generated summaries and reference summaries.

**Answer:**
**ROUGE-N:** Measures n-gram recall between generated and reference text, captures local coherence.

**ROUGE-L:** Uses longest common subsequence, captures sentence-level structure without requiring consecutive matches.

**Calculation:** ROUGE-N = (overlapping n-grams) / (total n-grams in reference)

**Interpretation:** Higher scores indicate better content coverage, but don't guarantee semantic quality.

## 12. How would you assess the informativeness and conciseness of a summarization model?

**Answer:**
**Informativeness:** Measure key information coverage, use abstractive content analysis, check fact preservation.

**Conciseness:** Calculate compression ratio, analyze redundancy, measure density of important information.

**Balanced Metrics:** Information density scores, compression-quality trade-offs, human evaluation of summary utility.

**Evaluation:** Expert assessment for domain-specific informativeness, user studies for practical utility.

## 13. How do you evaluate retrieval quality in RAG models, and why is it important?

**Answer:**
**Retrieval Metrics:** Recall@K, precision@K, MRR (Mean Reciprocal Rank), NDCG for ranking quality.

**Importance:** Poor retrieval leads to hallucinations, irrelevant responses, and reduced answer quality.

**Evaluation Process:** Create relevance judgments, measure retrieval accuracy, assess retrieved content quality.

**End-to-end Impact:** Correlate retrieval quality with final answer accuracy, measure retrieval's contribution to generation quality.

## 14. What strategies do you use to reduce hallucination in RAG models?

**Answer:**
**Retrieval Improvement:** Better embedding models, query expansion, hybrid search combining dense and sparse retrieval.

**Generation Control:** Source attribution requirements, confidence thresholds, fact-checking post-processing.

**Training Strategies:** Train on high-quality factual data, constitutional AI, uncertainty estimation.

**Evaluation:** Automated fact-checking, human verification, consistency checks across multiple generations.

## 15. How do you determine if fine-tuning has improved a model's performance on a specific task?

**Answer:**
**Baseline Comparison:** Compare fine-tuned model against pre-trained baseline using task-specific metrics.

**Statistical Significance:** Use proper train/validation/test splits, statistical tests for performance differences.

**Generalization:** Test on held-out data, cross-validation, evaluation on related but unseen tasks.

**Practical Improvement:** Measure real-world impact, user satisfaction, business metric improvements.

## 16. Discuss comparing baseline metrics with fine-tuned metrics, tracking loss curves, and using task-specific metrics to measure improvement.

**Answer:**
**Baseline Metrics:** Establish pre-training performance on task, zero-shot and few-shot baselines.

**Loss Tracking:** Monitor training and validation loss curves, detect overfitting, ensure convergence.

**Task Metrics:** Use domain-appropriate metrics (F1 for classification, BLEU for translation), multiple evaluation perspectives.

**Analysis:** Statistical significance testing, effect size measurement, practical significance assessment.

## 17. What challenges arise when fine-tuning large language models, and how do you mitigate them?

**Answer:**
**Catastrophic Forgetting:** Model loses general capabilities. **Mitigation:** Lower learning rates, regularization, elastic weight consolidation.

**Overfitting:** Poor generalization on small datasets. **Mitigation:** Data augmentation, early stopping, parameter-efficient fine-tuning.

**Resource Constraints:** Memory and compute limitations. **Mitigation:** LoRA, gradient checkpointing, mixed precision training.

**Data Quality:** Limited or biased training data. **Mitigation:** Data curation, diverse data sources, human evaluation.

## 18. Talk about overfitting, the need for robust validation datasets, and regularization techniques that ensure generalizability in fine-tuned models.

**Answer:**
**Overfitting Signs:** High training accuracy but poor validation performance, memorization of training examples.

**Robust Validation:** Large, diverse validation sets, multiple evaluation metrics, cross-validation approaches.

**Regularization:** Dropout, weight decay, early stopping, data augmentation, parameter-efficient methods.

**Generalizability:** Test on out-of-domain data, evaluate on related tasks, monitor performance over time.

## 19. How do you assess the quality of generated samples from a generative model?

**Answer:**
**Automated Metrics:** Inception Score (IS), Fréchet Inception Distance (FID) for images, perplexity for text.

**Human Evaluation:** Quality ratings, preference comparisons, expert assessment for domain-specific content.

**Diversity Measures:** Intra-class diversity, mode coverage, semantic diversity analysis.

**Task-specific:** Factual accuracy for informational content, creativity scores for artistic generation.

## 20. How would you set up an A/B test to evaluate two NLP models?

**Answer:**
**Experimental Design:** Random user assignment, control/treatment groups, sufficient sample size for statistical power.

**Metrics:** Primary KPIs (task accuracy, user satisfaction), secondary metrics (engagement, conversion).

**Controls:** Consistent experimental conditions, proper randomization, bias elimination.

**Analysis:** Statistical significance testing, confidence intervals, practical significance assessment, user experience impact.

## 21. Describe the importance of testing with a live audience, creating control/experimental groups, and using click-through rates or engagement metrics in addition to core NLP metrics.

**Answer:**
**Live Testing:** Real user behavior differs from lab settings, captures actual usage patterns and preferences.

**Group Design:** Proper randomization, balanced demographics, sufficient group sizes for statistical power.

**Engagement Metrics:** Click-through rates, time on task, user retention, conversion rates complement accuracy metrics.

**Holistic Evaluation:** Combine technical metrics with business outcomes, user satisfaction, and practical impact assessment.

## 22. How do latency and efficiency factor into evaluating NLP models, especially in production settings?

**Answer:**
**Latency Requirements:** Real-time applications need sub-second response times, batch processing can tolerate higher latency.

**Efficiency Metrics:** Throughput (requests/second), resource utilization, cost per inference, energy consumption.

**Trade-offs:** Balance accuracy with speed, optimize for production constraints, consider user experience impact.

**Optimization:** Model compression, caching, hardware acceleration, efficient serving infrastructure.

## 23. What's the role of explainability in NLP evaluation, especially for high-stakes applications?

**Answer:**
**Critical Importance:** High-stakes decisions require understanding of model reasoning, regulatory compliance, trust building.

**Explainability Methods:** Attention visualization, feature importance, counterfactual analysis, gradient-based explanations.

**Evaluation Criteria:** Explanation quality, faithfulness to model decisions, human interpretability, actionable insights.

**Applications:** Medical diagnosis, legal analysis, financial decisions where explanations are mandatory.

## 24. How do you measure user satisfaction with an NLP model deployed in a real-world application?

**Answer:**
**Direct Feedback:** User ratings, satisfaction surveys, explicit feedback collection.

**Behavioral Metrics:** Usage patterns, task completion rates, repeat usage, session duration.

**Implicit Signals:** Click patterns, correction frequency, abandonment rates, user retention.

**Longitudinal Analysis:** Track satisfaction over time, identify improvement areas, measure adaptation to user needs.

## 25. What is domain adaptation, and how do you evaluate it after fine-tuning a model on domain-specific data?

**Answer:**
Domain adaptation adjusts models trained on one domain to perform well on a different but related domain.

**Evaluation:** Compare performance on target domain before/after adaptation, measure transfer effectiveness.

**Metrics:** Target domain accuracy, retention of source domain capabilities, adaptation efficiency.

**Challenges:** Domain shift measurement, catastrophic forgetting prevention, generalization assessment.

## 26. How would you evaluate the robustness of an NLP model to adversarial attacks?

**Answer:**
**Attack Types:** Test against synonym substitution, paraphrase attacks, character-level perturbations, context manipulation.

**Robustness Metrics:** Attack success rate, model confidence under attack, performance degradation measurement.

**Evaluation Framework:** Systematic adversarial testing, red team evaluation, robustness benchmarks.

**Improvement:** Adversarial training, input validation, confidence thresholding, ensemble methods for increased robustness.

---

*These answers cover comprehensive evaluation methods for NLP and generative AI. Focus on understanding the trade-offs between different metrics and the importance of human evaluation for subjective tasks.*